{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "#from get_embedding_function import get_embedding_function\n",
    "from langchain.vectorstores.chroma import Chroma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scripts starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH='card_reviews.csv'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_community.embeddings.bedrock import BedrockEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "def get_embedding_function():\n",
    "    #embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    embeddings = HuggingFaceEmbeddings()\n",
    "    #embeddings = BedrockEmbeddings(\n",
    "    #    credentials_profile_name=\"default\", region_name=\"us-east-1\"\n",
    "    #)\n",
    "    # embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "def load_documents(FILE_PATH):\n",
    "    loader = CSVLoader(file_path=FILE_PATH,encoding='utf-8',csv_args={\n",
    "    'delimiter': ';',})\n",
    "    #document_loader = CSVLoader(DATA_PATH)\n",
    "    return loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.document import Document\n",
    "\n",
    "def split_documents(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=80,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_chunk_ids(chunks):\n",
    "\n",
    "    last_row_id = None\n",
    "    current_chunk_index = 0\n",
    "\n",
    "    for chunk in chunks:\n",
    "        source = chunk.metadata.get(\"source\")\n",
    "        row = chunk.metadata.get(\"row\")\n",
    "        current_row_id = f\"{source}:{row}\"\n",
    "\n",
    "        # If the row ID is the same as the last one, increment the index.\n",
    "        if current_row_id == last_row_id:\n",
    "            current_chunk_index += 1\n",
    "        else:\n",
    "            current_chunk_index = 0\n",
    "\n",
    "        # Calculate the chunk ID.\n",
    "        chunk_id = f\"{current_row_id}:{current_chunk_index}\"\n",
    "        last_row_id = current_row_id\n",
    "\n",
    "        # Add it to the row meta-data.\n",
    "        chunk.metadata[\"id\"] = chunk_id\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def clear_database():\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_batches(documents, batch_size):\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        yield documents[i:i + batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_to_chroma(chunks: list[Document]):\n",
    "    # Load the existing database.\n",
    "    db = Chroma(\n",
    "        persist_directory=CHROMA_PATH, embedding_function=get_embedding_function()\n",
    "    )\n",
    "\n",
    "    # Calculate row IDs.\n",
    "    chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "\n",
    "    # Add or Update the documents.\n",
    "    existing_items = db.get(include=[])  # IDs are always included by default\n",
    "    existing_ids = set(existing_items[\"ids\"])\n",
    "    print(f\"Number of existing documents in DB: {len(existing_ids)}\")\n",
    "\n",
    "    # Only add documents that don't exist in the DB.\n",
    "    new_chunks = []\n",
    "    for chunk in chunks_with_ids:\n",
    "        if chunk.metadata[\"id\"] not in existing_ids:\n",
    "            new_chunks.append(chunk)\n",
    "\n",
    "    if len(new_chunks):\n",
    "        print(f\" Adding new documents: {len(new_chunks)}\")\n",
    "        #new_chunk_ids = [chunk.metadata[\"id\"] for chunk in new_chunks]\n",
    "        max_batch_size = 160\n",
    "        for batch in split_into_batches(new_chunks, max_batch_size):\n",
    "            new_chunk_ids = [chunk.metadata[\"id\"] for chunk in batch]\n",
    "            db.add_documents(batch, ids=new_chunk_ids)\n",
    "            db.persist()\n",
    "        #db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "        #db.persist()\n",
    "    else:\n",
    "        print(\" No new documents to add\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of existing documents in DB: 0\n",
      "ðŸ‘‰ Adding new documents: 99\n"
     ]
    }
   ],
   "source": [
    "\n",
    "documents = load_documents(DATA_PATH)\n",
    "chunks = split_documents(documents)\n",
    "add_to_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "\n",
    "CHROMA_PATH = \"chroma\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Here is the similar customers from different companies provided card reviews. YOu are card advisor expert. Suggest cards based only on the customer reviews:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "There is a new customer that want to get new card. here is what customer expects from her/his card: {question}\n",
    "\"\"\"\n",
    "\n",
    "def query_rag(query_text: str):\n",
    "    # Prepare the DB.\n",
    "    embedding_function = get_embedding_function()\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "    # Search the DB.\n",
    "    results = db.similarity_search_with_score(query_text, k=5)\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    # print(prompt)\n",
    "\n",
    "    model = Ollama(model=\"llama3\")\n",
    "    response_text = model.invoke(prompt)\n",
    "\n",
    "    sources = [doc.metadata.get(\"id\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)\n",
    "    #return formatted_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: A new customer looking for a new card! Based on the reviews provided, here are some card recommendations that fit their expectations:\n",
      "\n",
      "1. **Low Interest Rate:** SBI ELITE (Foreign Exchange Markup @ 1.99% is a big advantage) seems to have a competitive interest rate, which would be suitable for this customer.\n",
      "\n",
      "2. **High Cashback:** ICICI RUBYX MASTERCARD has been praised for its good credit limit and offers not applicable in real-time. Although the review doesn't specifically mention cashback, it's possible that the card may offer some form of rewards or incentives.\n",
      "\n",
      "3. **Travel Rewards:** HDFC INFINIA was considered by a customer looking for a premium feel with travel acceptance (Master/Visa). This card might be suitable for someone who travels frequently and wants a card that can keep up with their needs.\n",
      "\n",
      "Considering these factors, I would recommend the following cards:\n",
      "\n",
      "* SBI ELITE: For its low interest rate and potential international usage.\n",
      "* HDFC INFINIA: For its premium feel and travel acceptance (Master/Visa).\n",
      "* ICICI RUBYX MASTERCARD: Although it doesn't specifically mention cashback, it has a good credit limit and might offer some form of rewards.\n",
      "\n",
      "Please note that these recommendations are based solely on the provided reviews and may not reflect the actual performance or features of these cards. It's always a good idea to do further research and consider your individual needs before making a decision.\n",
      "Sources: ['card_reviews.csv:76:0', 'card_reviews.csv:55:0', 'card_reviews.csv:95:0', 'card_reviews.csv:47:0', 'card_reviews.csv:21:0']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query_rag('I want a card with low interest rate and high cashback. Also I travel a lot so I want a card with travel rewards.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: As a card advisor expert, I'll analyze the reviews and suggest cards based on the customer's expectations.\n",
      "\n",
      "From the reviews provided:\n",
      "\n",
      "* HDFC JET PRIVILEGE is not known for its dining offers (only instant loans and good credit limit).\n",
      "* Kotak Pvr Gold has great dining and entertainment category rewards, but limited to only 10% of value spent, up to a maximum of â‚¹600 per month.\n",
      "* AXIS MILES AND MORE cards have varying opinions on the credit limit, interest rate, and cashback offers. However, none specifically mention restaurant-related benefits.\n",
      "\n",
      "Considering the customer's expectation: \"I am totally foodie. I would like to have a credit card that has offers or discounts for restaurants.\"\n",
      "\n",
      "Based on this, I would recommend Kotak Pvr Gold as it provides 10% of value spent on Dining and Entertainment categories, which aligns with the customer's interest.\n",
      "\n",
      "Please note that these recommendations are based solely on the provided reviews and may not include all aspects of a credit card. It's essential to consider other factors, such as annual fees, interest rates, and other benefits, before making a decision.\n",
      "Sources: ['card_reviews.csv:12:0', 'card_reviews.csv:80:0', 'card_reviews.csv:54:0', 'card_reviews.csv:50:0', 'card_reviews.csv:51:0']\n"
     ]
    }
   ],
   "source": [
    "query_rag('I am totally foodie. I would like to have a credit card that has offers or discounts for restaurants.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
